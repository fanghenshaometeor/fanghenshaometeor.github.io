---
permalink: /
title: "About"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm a Ph.D. student in [Institute of Image Processing and Pattern Recognition](http://www.pami.sjtu.edu.cn/) of [Department of Automation](https://automation.sjtu.edu.cn/) at [Shanghai Jiao Tong University](https://www.sjtu.edu.cn/). My advisor is [Prof. Jie Yang](https://scholar.google.com/citations?user=tmx7tu8AAAAJ&hl=en).

Before coming to SJTU, I received the BA.Eng. degree in [College of Electronic and Information Engineering](https://see.tongji.edu.cn/) from [Tongji University](https://www.tongji.edu.cn/) in 2018. I then received the MA.Eng. degree in [Department of Automation](https://automation.sjtu.edu.cn/) from [Shanghai Jiao Tong University](https://www.sjtu.edu.cn/) in 2021.


Research interests
---
My research interests focus on **trustworthy deep learning**. Currently, my works are about the output reliability from Deep Neural Networks (DNNs) against those *perturbed* or *shifted* inputs, within the fields of **adversarial robustness** and **out-of-distribution detection**.

- DNNs trained on clean images show terrible generalization performance on those carefully-designed invisible perturbed images, *i.e.*, *adversarial examples*, which has raised widespread attention on the *adversarial robustness* of DNNs.
- DNNs cannot generalize well on data that differs from the training distribution, *i.e.*, *In-Distribution (InD)*. In this regard, it remains a valuable task of detecting whether new samples are from the InD or OoD (Out-of-Distribution) of DNNs in the inference stage.

<!-- - **Adversarial robustness** and **pixel-level perturbations**

DNNs trained on clean images show weak generalization performance on those carefully-designed invisible perturbed images, *i.e.*, *adversarial examples*, which raises attention on the *adversarial robustness* of DNNs.
The process of generating adversarial examples is known as the *adversarial attack*, while *adversarial defenses* aim at improving the performance of DNNs on adversarial examples.
Prevailing white-box attacks rely on image gradients to generate pixel-level perturbations, which are added on clean images to create adversarial examples.
Prevalent adversarial defenses adopt the data augmentation technique by involving adversarial examples into the training data, so as to train more adversarially-robust DNNs.
It remains valuable tasks of devising powerful adversarial attacks to fool DNNs or enhanced adversarial defenses to strengthen DNNs, both of which would provide insights on better understandings of DNNs.

- **Out-of-distribution detection** and **distribution-level shifts**

In the inference or deployment stage, DNNs would inevitably encounter samples that differ from the training distribution of DNNs, *i.e.*, in-distribution (InD) -->



In addition, I have previously dabbled in the **random Fourier features**, a sub-field of the **kernel methods** in machine learning.

---
